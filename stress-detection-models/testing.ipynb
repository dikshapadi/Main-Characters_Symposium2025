{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f10c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce9dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and preprocessing objects.\n",
      "Warning: Dataset not found. Rolling statistics will be estimated for new users.\n",
      "\n",
      "=== Prediction for Single Row ===\n",
      "Predicted StressLevel: Medium\n",
      "Class Probabilities:\n",
      "High: 0.3484\n",
      "Low: 0.1505\n",
      "Medium: 0.5011\n",
      "\n",
      "Prediction saved to 'single_row_prediction.csv'.\n",
      "\n",
      "Testing on Single Row Completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Load Preprocessing Objects and Model\n",
    "try:\n",
    "    stacking_clf = joblib.load('stacking_classifier_model.pkl')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "    pca = joblib.load('pca.pkl')\n",
    "    label_encoders = joblib.load('label_encoders.pkl')\n",
    "    le_target = joblib.load('le_target.pkl')\n",
    "    print(\"Loaded model and preprocessing objects.\")\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Missing file: {e}. Ensure 'stacking_clf_model.pkl', 'scaler.pkl', 'pca.pkl', 'label_encoders.pkl', and 'le_target.pkl' are in the working directory.\")\n",
    "\n",
    "# Load dataset for rolling statistics context (optional if UserID exists)\n",
    "try:\n",
    "    df = pd.read_csv('final_synthetic_stress_dataset.csv')\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    print(\"Loaded dataset for rolling statistics.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Dataset not found. Rolling statistics will be estimated for new users.\")\n",
    "\n",
    "# Step 2: Define Features and Columns\n",
    "features = ['HR', 'HRV', 'SpO2', 'Steps', 'Distance', 'Calories', 'ActiveTime', 'SleepDuration', 'SleepEfficiency',\n",
    "            'Age', 'Sex', 'DrinkingHabits', 'SmokingHabits', 'PastMedicalHistory', 'Depression', 'Context',\n",
    "            'Hour', 'DayOfWeek', 'IsWeekend', 'TimeOfDay', 'HR_RollingMean', 'HR_RollingStd', 'HRV_RollingMean',\n",
    "            'HRV_RollingStd', 'SpO2_RollingMean', 'SpO2_RollingStd', 'HR_Steps_Interaction',\n",
    "            'HRV_SleepDuration_Interaction', 'ActivityIntensity']\n",
    "categorical_cols = ['Sex', 'DrinkingHabits', 'SmokingHabits', 'PastMedicalHistory', 'Depression', 'Context', 'TimeOfDay', 'ActivityIntensity']\n",
    "numerical_cols = [col for col in features if col not in categorical_cols]\n",
    "\n",
    "# Step 3: Prepare a Single Row of New Data\n",
    "# Example new data (replace with your actual row)\n",
    "new_data = {\n",
    "    'Timestamp': '2025-05-08 09:00:00',\n",
    "    'UserID': 'U999',\n",
    "    'HR': 85.0,\n",
    "    'HRV': 40.0,\n",
    "    'SpO2': 97.0,\n",
    "    'Steps': 50,\n",
    "    'Distance': 0.04,\n",
    "    'Calories': 3.5,\n",
    "    'ActiveTime': 1.0,\n",
    "    'SleepDuration': 7.5,\n",
    "    'SleepEfficiency': 90.0,\n",
    "    'Height': 170.0,\n",
    "    'Weight': 70.0,\n",
    "    'Age': 35,\n",
    "    'Sex': 'Male',\n",
    "    'DrinkingHabits': 'Occasional',\n",
    "    'SmokingHabits': 'Non-smoker',\n",
    "    'PastMedicalHistory': 'Other',\n",
    "    'Depression': 'No',\n",
    "    'Context': 'Work'\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_df = pd.DataFrame([new_data])\n",
    "\n",
    "# Validate input\n",
    "required_cols = ['Timestamp', 'UserID', 'HR', 'HRV', 'SpO2', 'Steps', 'Distance', 'Calories', 'ActiveTime',\n",
    "                'SleepDuration', 'SleepEfficiency', 'Height', 'Weight', 'Age', 'Sex', 'DrinkingHabits',\n",
    "                'SmokingHabits', 'PastMedicalHistory', 'Depression', 'Context']\n",
    "if not all(col in new_df.columns for col in required_cols):\n",
    "    raise ValueError(f\"Missing required columns. Required: {required_cols}\")\n",
    "\n",
    "# Step 4: Preprocess the Single Row\n",
    "# Feature Engineering\n",
    "new_df['Timestamp'] = pd.to_datetime(new_df['Timestamp'])\n",
    "new_df['Hour'] = new_df['Timestamp'].dt.hour\n",
    "new_df['DayOfWeek'] = new_df['Timestamp'].dt.dayofweek\n",
    "new_df['IsWeekend'] = new_df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "new_df['TimeOfDay'] = pd.cut(new_df['Hour'], bins=[0, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'], include_lowest=True)\n",
    "\n",
    "# Rolling Statistics\n",
    "window = 6\n",
    "user_id = new_df['UserID'].iloc[0]\n",
    "if 'df' in globals() and user_id in df['UserID'].values:\n",
    "    user_history = df[df['UserID'] == user_id][['Timestamp', 'HR', 'HRV', 'SpO2']].copy()\n",
    "    user_history = pd.concat([user_history, new_df[['Timestamp', 'HR', 'HRV', 'SpO2']]], ignore_index=True)\n",
    "    for col in ['HR', 'HRV', 'SpO2']:\n",
    "        new_df[f'{col}_RollingMean'] = user_history[col].rolling(window=window, min_periods=1).mean().iloc[-1]\n",
    "        new_df[f'{col}_RollingStd'] = user_history[col].rolling(window=window, min_periods=1).std().iloc[-1]\n",
    "else:\n",
    "    # Fallback: Estimate rolling statistics\n",
    "    try:\n",
    "        for col in ['HR', 'HRV', 'SpO2']:\n",
    "            new_df[f'{col}_RollingMean'] = df[col].mean()\n",
    "            new_df[f'{col}_RollingStd'] = df[col].std()\n",
    "    except NameError:\n",
    "        # If dataset is unavailable, use reasonable defaults\n",
    "        defaults = {'HR': (70.0, 15.0), 'HRV': (50.0, 15.0), 'SpO2': (97.0, 1.5)}\n",
    "        for col in ['HR', 'HRV', 'SpO2']:\n",
    "            new_df[f'{col}_RollingMean'] = defaults[col][0]\n",
    "            new_df[f'{col}_RollingStd'] = defaults[col][1]\n",
    "        print(\"Warning: Dataset unavailable. Using default values for rolling statistics.\")\n",
    "\n",
    "# Interaction Features\n",
    "new_df['HR_Steps_Interaction'] = new_df['HR'] * new_df['Steps']\n",
    "new_df['HRV_SleepDuration_Interaction'] = new_df['HRV'] * new_df['SleepDuration']\n",
    "new_df['ActivityIntensity'] = pd.cut(new_df['Steps'], bins=[-1, 100, 400, 600], labels=['Low', 'Moderate', 'High'], include_lowest=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "X_new = new_df[features]\n",
    "for col in categorical_cols:\n",
    "    try:\n",
    "        X_new[col] = label_encoders[col].transform(X_new[col])\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid value for {col}. Must be one of {label_encoders[col].classes_}\")\n",
    "\n",
    "# Scale numerical features\n",
    "X_new[numerical_cols] = scaler.transform(X_new[numerical_cols])\n",
    "\n",
    "# Apply PCA\n",
    "X_new_pca = pca.transform(X_new[numerical_cols])\n",
    "X_new_pca_df = pd.DataFrame(X_new_pca, columns=[f'PC{i+1}' for i in range(X_new_pca.shape[1])])\n",
    "\n",
    "# Combine PCA components with categorical features\n",
    "X_new_final = pd.concat([X_new_pca_df, X_new[categorical_cols].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Step 5: Predict with Stacking Classifier\n",
    "y_pred = stacking_clf.predict(X_new_final)\n",
    "y_proba = stacking_clf.predict_proba(X_new_final)\n",
    "\n",
    "# Decode prediction\n",
    "predicted_class = le_target.inverse_transform(y_pred)[0]\n",
    "proba_dict = {le_target.inverse_transform([i])[0]: prob for i, prob in enumerate(y_proba[0])}\n",
    "\n",
    "print(\"\\n=== Prediction for Single Row ===\")\n",
    "print(f\"Predicted StressLevel: {predicted_class}\")\n",
    "print(\"Class Probabilities:\")\n",
    "for class_name, prob in proba_dict.items():\n",
    "    print(f\"{class_name}: {prob:.4f}\")\n",
    "\n",
    "# Step 6: Save Results\n",
    "new_df['Predicted_StressLevel'] = predicted_class\n",
    "new_df['Prob_Low'] = proba_dict['Low']\n",
    "new_df['Prob_Medium'] = proba_dict['Medium']\n",
    "new_df['Prob_High'] = proba_dict['High']\n",
    "new_df.to_csv('single_row_prediction.csv', index=False)\n",
    "print(\"\\nPrediction saved to 'single_row_prediction.csv'.\")\n",
    "\n",
    "print(\"\\nTesting on Single Row Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
